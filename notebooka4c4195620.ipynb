{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14535339,"sourceType":"datasetVersion","datasetId":9283586}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install langchain-community langchain langgraph langchain-groq langchain-google-genai google-generativeai google-api-python-client google-auth-httplib2 google-auth-oauthlib wikipedia faiss-cpu tiktoken pypdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T09:04:34.725824Z","iopub.execute_input":"2026-01-18T09:04:34.726199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom google.colab import userdata\nfrom typing import List, Literal, Dict, Any, Optional\nfrom typing_extensions import TypedDict\n# --- Core Langchain & Langgraph Imports ---\nfrom langchain_core.messages import AIMessage, HumanMessage, BaseMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain.schema import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\nfrom langchain_community.utilities import WikipediaAPIWrapper\nfrom langchain_community.tools import WikipediaQueryRun\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\nfrom langgraph.graph import END, StateGraph, START","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nos.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY \nprint(\"Google API key loaded from Kaggle secrets.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Configuration ---\nDOCS_PATH = \"/kaggle/input/poldocs\"  # Create this directory and put policy .pdf files inside\nFAISS_INDEX_PATH = \"/kaggle/working/hr-policy-faiss-index-pdf/\"\nLLM_MODEL_NAME = \"gemini-1.5-flash-latest\" \nEMBEDDING_MODEL_NAME = \"models/embedding-001\" # Standard Google embedding model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_community.document_loaders import PyPDFLoader\ndef create_or_load_vector_store():\n    \"\"\"Loads PDF documents, creates embeddings, and builds/loads a FAISS index.\"\"\"\n    if os.path.exists(FAISS_INDEX_PATH):\n        print(f\"Loading existing FAISS index from: {FAISS_INDEX_PATH}\")\n        embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL_NAME, google_api_key=os.environ['GOOGLE_API_KEY'])\n        vectorstore = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n        print(\"FAISS index loaded.\")\n        return vectorstore.as_retriever(search_kwargs={\"k\": 3})\n    else:\n        print(f\"Creating new FAISS index. Loading PDF documents from: {DOCS_PATH}\")\n        # Load documents using PyPDFLoader\n        # Use recursive=True if PDFs might be in subdirectories\n        loader = DirectoryLoader(\n            DOCS_PATH,\n            glob=\"**/*.pdf\",   # Look for PDF files\n            loader_cls=PyPDFLoader, # Use the PDF loader\n            show_progress=True,\n            use_multithreading=True # Can speed up loading multiple PDFs\n        )\n        docs = loader.load()\n        if not docs:\n            # Changed error message to reflect PDF expectation\n            raise FileNotFoundError(f\"No .pdf files found in {DOCS_PATH}. Please place policy PDF documents there.\")\n\n        # Split documents (remains the same)\n        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=50)\n        docs_split = text_splitter.split_documents(docs)\n        # Check content after splitting (PDF loading can sometimes yield empty pages)\n        docs_split = [doc for doc in docs_split if doc.page_content.strip()]\n        if not docs_split:\n             raise ValueError(f\"PDF files in {DOCS_PATH} were loaded, but            resulted in no text content after splitting.Check PDF content/format.\")\n        print(f\"Split {len(docs)} PDF pages/documents into {len(docs_split)}            text chunks.\")\n\n        # Create embeddings (remains the same)\n        print(f\"Initializing Google Embeddings: {EMBEDDING_MODEL_NAME}\")\n        embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL_NAME,         google_api_key=os.environ['GOOGLE_API_KEY'])\n\n        # Create FAISS index (remains the same)\n        print(\"Creating FAISS vector store...\")\n        vectorstore = FAISS.from_documents(docs_split, embeddings)\n\n        # Save FAISS index (remains the same)\n        print(f\"Saving FAISS index to: {FAISS_INDEX_PATH}\")\n        vectorstore.save_local(FAISS_INDEX_PATH)\n        print(\"FAISS index created and saved.\")\n        return vectorstore.as_retriever(search_kwargs={\"k\": 3})\n        retriever = create_or_load_vector_store()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Tool Setup (Wikipedia Only) ---\nprint(\"Setting up Wikipedia tool...\")\napi_wrapper_wiki = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=2000)\nwiki_tool = WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\nprint(\"Wikipedia tool initialized.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LLM Setup (Using ChatGoogleGenerativeAI) ---\nprint(f\"Initializing LLM: {LLM_MODEL_NAME}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's configure it generally first. Specific tool binding happens later if needed.\nllm = ChatGoogleGenerativeAI(model=LLM_MODEL_NAME,\n                             temperature=0,\n                             convert_system_message_to_human=True # Often helpful for Gemini\n                            )\nprint(\"LLM initialized.\")\nOutput:\nLLM initialized.\n\n\nGraph State Definition ---\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: The current user question.\n        generation: The LLM generation.\n        documents: List of documents retrieved from any source.\n        chat_history: List of BaseMessages for conversation memory.\n        datasource: The source chosen for the final answer ('vectorstore', 'wikipedia', 'google', 'fallback').\n    \"\"\"\n    question: str\n    generation: Optional[str] = None\n    documents: Optional[List[str]] = None\n    chat_history: Optional[List[BaseMessage]] = None\n    datasource: Optional[Literal['vectorstore', 'wikipedia', 'google_grounded', 'fallback']] = None # Renamed 'google' source\n\nNode Functions ---\n\nRetrieve from Local Vector Store\ndef retrieve_local(state: GraphState) -> Dict[str, Any]:\n    \"\"\"Retrieve documents from the local FAISS vector store.\"\"\"\n    print(\"---NODE: retrieve_local---\")\n    question = state[\"question\"]\n    print(f\"Retrieving documents for: {question}\")\n    # Assuming retriever was initialized globally\n    retrieved_docs = retriever.invoke(question)\n    doc_contents = [doc.page_content for doc in retrieved_docs]\n    print(f\"Retrieved {len(doc_contents)} documents locally.\")\n    return {\"documents\": doc_contents, \"question\": question} # Pass question along for clarity if needed\n\nGrade Local Document Relevance (using LLM)\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for relevance check.\"\"\"\n    binary_score: Literal[\"yes\", \"no\"] = Field(..., description=\"Is the document relevant to the question, considering chat history? 'yes' or 'no'\")\n\ndef grade_documents(state: GraphState) -> Dict[str, Any]:\n    \"\"\"Determines whether the retrieved documents are relevant to the question and history.\"\"\"\n    print(\"---NODE: grade_documents---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    chat_history = state.get(\"chat_history\", []) # Get history or empty list\n\n    if not documents:\n        print(\"No documents retrieved, grading as 'no'.\")\n        return {\"documents\": documents, \"datasource\": None, \"grade\": \"no\"} # Indicate no relevant source yet\n\n    # Use LLM to grade relevance\n    structured_llm_grader = llm.with_structured_output(GradeDocuments)\n    \n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"You are a grader assessing relevance of retrieved documents to a user question, considering the conversation history. Documents contain HR policies. Answer 'yes' if the documents likely contain the answer, 'no' otherwise.\"),\n        MessagesPlaceholder(variable_name=\"chat_history\"),\n        (\"human\", \"Retrieved documents:\\n\\n{documents}\\n\\nUser question: {question}\")\n    ])\n    \n    # Format documents for the prompt\n    doc_string = \"\\n\\n---\\n\\n\".join(documents)\n    \n    #Create the full chain first ***\n    grader_chain = prompt | structured_llm_grader\n    \n    print(\"Asking LLM to grade document relevance...\")\n    #Invoke the chain, not just the structured LLM ***\n    response = grader_chain.invoke({\n        \"question\": question,\n        \"documents\": doc_string,\n        \"chat_history\": chat_history\n    })\n    \n    print(f\"Relevance Grade: {response.binary_score}\")\n    if response.binary_score == \"yes\":\n        return {\"documents\": documents, \"datasource\": \"vectorstore\", \"grade\": \"yes\"}\n    else:\n        # Documents are not relevant, clear them and decide next step\n        print(\"Documents deemed irrelevant by grader.\")\n        return {\"documents\": None, \"datasource\": None, \"grade\": \"no\"} # Clear docs, no source decided yet\n\n\nRoute to Web Search or Fallback \n# Decision options are now 'wikipedia', 'search_generate', 'fallback'\n\nclass RouteChoice(BaseModel):\n    \"\"\"Route decision.\"\"\"\n    datasource: Literal[\"wikipedia\", \"search_generate\", \"fallback\"] = Field(..., description=\"Given the user question and chat history, decide whether to search Wikipedia, perform a grounded Google search ('search_generate'), or if the question is unanswerable/absurd ('fallback').\")\n\ndef route_web_or_fallback(state: GraphState) -> Dict[str, str]:\n    \"\"\"Routes to Wiki, Grounded Search (via Gemini), or Fallback.\"\"\"\n    print(\"---NODE: route_web_or_fallback---\")\n    question = state[\"question\"]\n    chat_history = state.get(\"chat_history\", [])\n    print(f\"Routing question: {question}\")\n\n    # Use the main 'llm' instance for routing decision\n    structured_llm_router = llm.with_structured_output(RouteChoice)\n\n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"\"\"You are an expert router. The user asked a question about HR policies.\n        We already checked a local vector store with policies on onboarding, separation, leaves, and connect, but found nothing relevant.\n        Based on the question and conversation history:\n        - If it looks like a standard factual query that Wikipedia might answer (e.g., general knowledge, definitions), choose 'wikipedia'.\n        - If it seems like a query requiring up-to-date information or broader context best answered by Google Search, choose 'search_generate'.\n        - If the question seems nonsensical, unrelated to HR, or unanswerable even with web search, choose 'fallback'.\"\"\"),\n        MessagesPlaceholder(variable_name=\"chat_history\"),\n        (\"human\", \"{question}\")\n    ])\n    \n    # Create the full chain first ***\n    router_chain = prompt | structured_llm_router\n    \n    #Invoke the chain, not just the structured LLM ***\n    route_result = router_chain.invoke({\n        \"question\": question,\n        \"chat_history\": chat_history\n    })\n    \n    print(f\"Routing Decision: {route_result.datasource}\")\n    # Return key matches the conditional edge keys later\n    return {\"datasource_decision\": route_result.datasource}\n\nWikipedia Search\ndef wiki_search(state: GraphState) -> Dict[str, Any]:\n    \"\"\"Perform Wikipedia search.\"\"\"\n    print(\"---NODE: wiki_search---\")\n    question = state[\"question\"]\n    print(f\"Searching Wikipedia for: {question}\")\n    search_result = wiki_tool.invoke({\"query\": question})\n    print(f\"Wikipedia Result Length: {len(search_result)}\")\n    return {\"documents\": [search_result], \"datasource\": \"wikipedia\"} # Wrap in list, set source\n\nGrounded Search and Generate\ndef search_and_generate(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Generates an answer using the Gemini LLM, allowing it to use its\n    built-in Google Search tool (grounding) if needed.\n    \"\"\"\n    print(\"---NODE: search_and_generate (using Gemini Grounding)---\")\n    question = state[\"question\"]\n    chat_history = state.get(\"chat_history\", [])\n\n    # The core idea: Give Gemini the question and history, and instruct it\n    # to answer, implicitly allowing it to use search if it deems necessary.\n    # LangChain's ChatGoogleGenerativeAI handles the tool call automatically\n    # when the model decides to use its internal search tool.\n\n    prompt = ChatPromptTemplate.from_messages([\n         (\"system\", \"\"\"You are an HR assistant chatbot. Answer the user's question based on the conversation history and your general knowledge.\n         If you need to find current information or information not available in the history, use your search capabilities.\n         Provide a concise and accurate response. If you cannot find a relevant answer even after searching, state that.\"\"\"),\n         MessagesPlaceholder(variable_name=\"chat_history\"),\n         (\"human\", \"{question}\"),\n    ])\n\n    # Use the main llm instance. It's configured to potentially use tools.\n    chain = prompt | llm\n\n    print(\"Generating answer with potential grounding...\")\n    response = chain.invoke({\n        \"question\": question,\n        \"chat_history\": chat_history\n        })\n\n    generation = response.content\n    print(\"Generation complete (with potential grounding).\")\n    # Indicate that Google grounding was the potential source\n    return {\"generation\": generation, \"datasource\": \"google_grounded\"}\n\n\nGenerate Final Answer\ndef generate(state: GraphState) -> Dict[str, Any]:\n    \"\"\"Generates the final answer using LLM, context, and history.\"\"\"\n    print(\"---NODE: generate---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    chat_history = state.get(\"chat_history\", [])\n    datasource = state[\"datasource\"]\n\n    if not documents:\n        print(\"Generation node called with no documents. This shouldn't happen if routing is correct. Triggering fallback.\")\n        # This case should ideally be caught earlier, but as a safeguard:\n        return fallback(state) # Call the fallback logic directly\n\n    # Format documents and history for the prompt\n    context = \"\\n\\n---\\n\\n\".join(documents)\n\n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", f\"\"\"You are an HR assistant chatbot. Answer the user's question related to Travel, Leave, annual health checks, notice period, office timings and separation policies based *only* on the provided context and conversation history.\n        The context provided is from: {datasource}.\n        Be concise and accurate. If the context does not contain the answer, ask Specific questions based on the contexts or explicitly state that you cannot answer based on the provided information.\n        Do not make information up.\n        <context>\n        {{context}}\n        </context>\"\"\"),\n        MessagesPlaceholder(variable_name=\"chat_history\"),\n        (\"human\", \"{question}\"),\n    ])\n\n    chain = prompt | llm\n\n    print(\"Generating final answer...\")\n    response = chain.invoke({\n        \"question\": question,\n        \"context\": context,\n        \"chat_history\": chat_history\n        })\n\n    generation = response.content\n    print(\"Generation complete.\")\n    # Return only the generation and final datasource\n    return {\"generation\": generation, \"datasource\": datasource}\n\n\n\n7. Final Generation and Absurdity Handling\nOnce all tools have done their job, the LLM either generates a confident response or politely communicates that no answer was found. This ensures we avoid misleading replies and maintain a high level of trust with users.\n\nIf a strong context is found, the LLM constructs a direct, well-informed response. If not, the bot shares a fallback message like:\n\n\"Sorry, I couldnâ€™t find enough information to answer that. Could you rephrase or contact HR directly?\"\n\nThis avoids hallucinations and maintains trust.\n\n\n\nFallback Answer\ndef fallback(state: GraphState) -> Dict[str, Any]:\n    \"\"\"Generates a fallback response if no relevant info is found or the question is absurd.\"\"\"\n    print(\"---NODE: fallback---\")\n    generation = \"I couldn't find relevant information in our HR policies or via web search to answer your question accurately. If your query is about HR policies, please try rephrasing. Otherwise, it might be best to contact the HR department directly.\"\n    return {\"generation\": generation, \"datasource\": \"fallback\"}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 6. Build the Graph ---\nprint(\"\\nBuilding graph...\")\nworkflow = StateGraph(GraphState)\n# Add Nodes (Replace google_search with search_and_generate)\nworkflow.add_node(\"retrieve_local\", retrieve_local)\nworkflow.add_node(\"grade_documents\", grade_documents)\nworkflow.add_node(\"route_web_or_fallback\", route_web_or_fallback)\nworkflow.add_node(\"wiki_search\", wiki_search)\nworkflow.add_node(\"search_and_generate\", search_and_generate)\nworkflow.add_node(\"generate\", generate)\nworkflow.add_node(\"fallback\", fallback)\n# Define Edges\n\n# Start -> Try local retrieval first\nworkflow.add_edge(START, \"retrieve_local\")\n\n# After local retrieval -> Grade the documents\nworkflow.add_edge(\"retrieve_local\", \"grade_documents\")\n\n# Conditional edge based on grading\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    lambda x: x.get(\"grade\", \"no\"),\n    {\n        \"yes\": \"generate\", # If relevant local docs, generate from them\n        \"no\": \"route_web_or_fallback\", # If not relevant, decide web/fallback\n    },\n)\n\n# Conditional edge for web/fallback routing (MODIFIED)\nworkflow.add_conditional_edges(\n    \"route_web_or_fallback\",\n    lambda x: x.get(\"datasource_decision\"), # Check the 'datasource_decision' field\n    {\n        \"wikipedia\": \"wiki_search\",\n        \"search_generate\": \"search_and_generate\", # Route to the new node\n        \"fallback\": \"fallback\",\n    }\n)\n\n# After Wikipedia search -> Generate answer from Wiki context\nworkflow.add_edge(\"wiki_search\", \"generate\")\n\n# After Grounded Search/Generate -> END (The node itself generates the answer)\nworkflow.add_edge(\"search_and_generate\", END) # Directly to END\n\n# Generate from local/wiki context -> END\nworkflow.add_edge(\"generate\", END)\n\n# Fallback -> END\nworkflow.add_edge(\"fallback\", END)\n\n# Compile the graph\napp = workflow.compile()\nprint(\"Graph compiled successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Example Invocation with History ---\nprint(\"\\n--- Running Chat ---\")\n\n# Initialize chat history (outside the loop)\nchat_history = []\n\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n        print(\"Chatbot: Goodbye!\")\n        break\n\n    # Prepare graph input, including history\n    inputs = {\"question\": user_input, \"chat_history\": chat_history}\n\n    # Invoke the graph\n    print(\"\\nChatbot thinking...\")\n    final_state = app.invoke(inputs) # Get the final state after execution\n\n    # Extract the final generation\n    final_generation = final_state.get(\"generation\", \"Sorry, something went wrong.\")\n    print(f\"Chatbot: {final_generation}\")\n\n    # Update chat history\n    chat_history.append(HumanMessage(content=user_input))\n    chat_history.append(AIMessage(content=final_generation))\n\n    # Optional: Limit history size\n    if len(chat_history) > 10: # Keep last 5 turns (10 messages)\n        chat_history = chat_history[-10:]\n\n    print(\"-\" * 30) # Separator for turns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}